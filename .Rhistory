colnames(data)
# Descriptive Statistics
summary(data)
h = dim(data$satisfaction_satisfied)
h
h = dim(data)
print(h)
print(h[0])
print(h(1))
h = dim(data[,1])
print(h)
h = dim(data[1])
h
h = dim(data[1,1])
h = dim(data[0])
h = dim(data[0])
h
y = h-4
print(y)
typeof(h)
?dim
dim(data)[0]
dim(data[1])
dim(data[2])
as.numeric(dim(data))
i <- as.numeric(dim(data))
i[0]
o[1,]
i[,1]
i[0,]
print(i)
i(1,)
colnames(data)
test <- read.csv("~/BA749/test.csv")
train <- read.csv("~/BA749/train.csv")
# view column names
colnames(test)
colnames(train)
# combine the datasets together using union
df <- union_all(test, train)
# view total number of columns
ncol(df)
# remove nominal/id predictors
colnames(df)
df <- df[-c(1:2)]
colnames(df)
# Ratio of satisfation
dim(data)
h=dim(data)
h[1]
[dim(data),1]
dim(data[1])
dim(data[1,0])
rows = dim(data)
rows = rows[1]
rows
?count
count(data$satisfaction_satisfied)
table(data$satisfaction_satisfied)
table(data$satisfaction_satisfied[1])
table(data$satisfaction_satisfied[0])
table(data$satisfaction_satisfied[1,2])
table(data$satisfaction_satisfied[1,1])
table = table(data$satisfaction_satisfied)
table[1]
unsat_ratio = table(data$satisfaction_satisfied)
unsat_ratio = table[1]
sat_ratio = table[2]
unsat_ratio = unsat_ratio/rows
sat_ratio = 1 - unsat_ratio
unsat_ratio
sat_ratio
table(data$Age)
?ggpie
??ggpie
?geom_density
# Visualizations
geom_density(data=data, geom="area")
# Visualizations
geom_density(data=data)
# Visualizations
geom_density(data=data$Age)
# Visualizations
ggplot(data, aes(x=data$Age))
# Visualizations
age = data$Age
ggplot(data, aes(x=age))
??ggplot
?geom_density
ggplot(data, aes(x=age)) + geom_density()
ggplot(data, aes(x=customer.type)) + geom_density()
ggplot(data, aes(x=Customer.Type)) + geom_density()
colnames(data)
distance = data$Flight.Distance
ggplot(data, aes(x=distance) + geom_density()
distance = data$Flight.Distance
ggplot(data, aes(x=distance)) + geom_density()
inflight = data$Inflight.service
ggplot(data, aes(x=inflight)) + geom_density()
colnames(df)
gender = df$Gender
ggplot(df, aes(x=gender)) + geom_density()
table(df$Gender)
table(data$Gender_Male)
gender = data$Gender_Male
ggplot(df, aes(x=gender)) + geom_density()
ggplot(data, aes(x=age)) + geom_density()
ggplot(data, aes(x=age)) + geom_density(aes(y = ..count..)
ggplot(data, aes(x=age)) + geom_density(aes(y = ..count..))
ggplot(data, aes(x=age)) + geom_density(aes(y = ..count..))
ggplot(data, aes(x=distance)) + geom_density(aes(y = ..count..))
table(distance)
ggplot(data, aes(x=age)) + geom_density
ggplot(data, aes(x=age)) + geom_density()
ggplot(data, aes(x=distance)) + geom_density()
?purrr
install.packages("tidyverse")
require(tidyverse)
%>%
?keep
?gather
data %>% keep(is.numeric) %>%
pivot_longer() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
?keep
data
head(data)
dim(data)
rows
facet
?facet_wrap
d = data
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_density()
?geom_smooth
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_density() +
geom_smooth(method = lm)
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_smooth(method = lm)
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_density()
?geom_density
?facet
?facet_wrap
table(data$Checkin.service)
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_bar()
colnames(data)
data$X <- NULL
# Descriptive Statistics
summary(data)
# Demographic Information
table(data$Age) #
table(data$Gender_Male) # 0 = Female, 1 = Male
d = data
d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_bar()
?printcp
pdf('visualizations_mis749.pdf')
plot(d %>%
keep(is.numeric) %>%                     # Keep only numeric columns
gather() %>%                             # Convert to key-value pairs
ggplot(aes(value)) +                     # Plot the values
facet_wrap(~ key, scales = "free") +   # In separate panels
geom_bar() )
dev.off()
library(ISLR)
library(gam)
library(visreg)
install.packages(c("gam","visreg"))
install.packages(c("gam", "visreg"))
data <- read.csv("~/mis749_cleaned.csv")
data <- read.csv("~/mis749_cleaned.csv")
data$X <- NULL
# Create training and test data set
set.seed(199)
trainIndex <- createDataPartition(data$satisfaction_satisfied, p=.7, list=F)
d.train <- data$satisfaction_satisfied[trainIndex,]
d.train <- data[trainIndex,]
d.test <- data[-trainIndex,]
?trainControl
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(data$satisfaction_satisfied ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
?train.default
table(data$satisfaction_satisfied)
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
trainControl()
?trainControl
View(data(Default))
data("Default")
force(Default)
# Relevel satisfaction variable
data$satisfaction_satisfied <- relevel(data$satisfaction_satisfied, ref=1)
#converting to factors to dummy codes
#including DV because it should be a binary value too
data <- as.data.frame(data)
View(data)
# Create training and test data set
set.seed(199)
trainIndex <- createDataPartition(data$satisfaction_satisfied, p=.7, list=F)
d.train <- data[trainIndex,]
d.test <- data[-trainIndex,]
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=F, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
y
?train
d.log <-  train(data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
d.log <-  train(x=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
d.log <-  train(x=data, y=y, method="glm", family="binomial", metric="ROC", trControl=ctrl)
data$satisfaction_satisfied <- factor(data$satisfaction_satisfied)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
y = data$satisfaction_satisfied
d.log <-  train(x=data, y=y, method="glm", family="binomial", metric="ROC", trControl=ctrl)
d.log <-  train(y~., method="glm", family="binomial", metric="ROC", trControl=ctrl)
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
?make.names
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl, verbose=FALSE)
levels(data$satisfaction_satisfied) <- c('satisfied', 'unsatisfied')
data
y = data$satisfaction_satisfied
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl, verbose=FALSE)
verbose=FALSE
summary(d.log)
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
warnings()
summary(d.log)
data
d.log <-  train(y ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
d.log <-  train(satisfaction_satisfied ~ ., data=data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
varImp(d.log)
d.log
#calculate resampled accuracy/confusion matrix using extracted predictions from resampling
confusionMatrix(d.log$pred$pred, d.log$pred$obs) #take averages
# KNN Classifier
set.seed(199)
d.knn <-  train(satisfaction_satisfied ~ ., data=data, method="knn", metric="ROC", trControl=ctrl, tuneLength=10) #let caret decide 10 best parameters to search
test <- read.csv("~/BA749/test.csv")
train <- read.csv("~/BA749/train.csv")
# view column names
colnames(test)
colnames(train)
# combine the datasets together using union
df <- union_all(test, train)
# view total number of columns
ncol(df)
# remove nominal/id predictors
colnames(df)
df <- df[-c(1:2)]
colnames(df)
# view data types of df
str(df)
colnames(Df)
colnames(df)
write.csv('data_uncleaned.csv')
write.csv(df, 'data_uncleaned.csv')
dim(df)
dim(data)
colnames(data)
colnames(df)
df$Gender <-NULL
df$Arrival.Delay.in.Minutes <- NULL
df$Departure.Delay.in.Minutes <- NULL
colnames(df)
colnames(data)
df$Inflight.wifi.service <- NULL
colnames(df)
write.csv(df, 'data_uncleaned.csv')
library(pROC)
require(dplyr)
require(caret)
data <- read.csv("~/PycharmProjects/bda_594/BDA594-Group10/cleaned_data.csv", header=TRUE)
df <- data
colnames(df)
dim(df)
# remove first column
data <- data[-1]
colnames(data)
df <- data
########## remove outliers from predictors ##########
# remove temp < 75 or > 108
temp_range <- (75:108)
data <- subset(data, data$temperature %in% temp_range)
# remove resprate > 100 or < 10
resp_rate <- (10:100)
data <- subset(data, data$resprate %in% resp_rate)
# remove O2 state below 60
o2_rate <- (60:1000)
data <- subset(data, data$o2sat %in% o2_rate)
# remove pain levels outside of 0-10 range
pain_range <- (0:10)
data <- subset(data, data$pain %in% pain_range)
# compare descriptive stats before and after
summary(df) # original dataset
summary(data)
removed_rows = dim(df[1]) - dim(data[1])
removed_rows <- total_rows[1]
removed_rows <- removed_rows[1]
removed_rows
data <- read.csv("~/cleaned_data2.csv", header=TRUE)
# compare descriptive stats before and after
summary(df) # original dataset
summary(data)
removed_rows = dim(df[1]) - dim(data[1])
removed_rows
## write a new csv file of cleaned dataset
write.csv(data, 'cleaned_data2.csv', row.names = FALSE)
data <- read.csv("~/cleaned_data2.csv", header=TRUE)
View(data)
# replace values on acuity column with yes or no........
data$acuity[data$acuity == 0] <- "No"
data$acuity[data$acuity == 1] <- "Yes"
# create factor of 2 levels
data$acuity <- as.factor(data$acuity)
set.seed(199)
trainIndex <- createDataPartition(data$acuity, p = .7, list=F)
train_data <- data[trainIndex,]
test_data <- data[-trainIndex,]
# checking for imbalances
d.table<- table(data$acuity)
prop.table(d.table)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
# simple additive logistic regression ON TRAINING DATA
set.seed(199)
default_glm_method = train(acuity ~ ., train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(default_glm_method)
train
train()
?train
summary(default_glm_method)
varImp(default_glm_method)
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(acuity ~ ., data=train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(acuity ~ ., data=train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
varImp(d.log)
d.log
#calculate resampled accuracy/confusion matrix using extracted predictions from resampling
confusionMatrix(d.log$pred$pred, d.log$pred$obs) #take averages
##linear discriminant analysis
set.seed(199)
d.lda <-  train(default ~ ., data=d.train, method="lda", metric="ROC", trControl=ctrl)
d.lda <-  train(acuity~ ., data=train_data, method="lda", metric="ROC", trControl=ctrl)
d.lda
varImp(d.lda)
confusionMatrix(d.lda$pred$pred, d.lda$pred$obs) #take averages
#k nearest neighbors classification
set.seed(199)
d.knn <-  train(default ~ ., data=d.train, method="knn", metric="ROC", trControl=ctrl, tuneLength=10) #let caret decide 10 best parameters to search
d.knn <-  train(acuity ~ ., data=train_data, method="knn", metric="ROC", trControl=ctrl, tuneLength=10) #let caret decide 10 best parameters to search
data$acuity
train_data$acuity
(d.lda)
(d.lda)
typeOf(train_data$acuity)
type(train_data$acuity)
typeof(train_data$acuity)
dim(train_data$acuity)
dim(train_data)
setwd("~/PycharmProjects/bda_594/BDA594-Group10")
install.packages("dplyr")
install.packages("dplyr")
library(pROC)
require(dplyr)
require(caret)
#data <- read.csv("~/cleaned_data2.csv", header=TRUE)
data <- read.csv("cleaned_data2.txt")
data <- read.csv("~/cleaned_data2.csv", header=TRUE)
#data <- read.csv("cleaned_data2.txt")
View(data)
# replace values on acuity column with yes or no........
data$acuity[data$acuity == 0] <- "No"
data$acuity[data$acuity == 1] <- "Yes"
# create factor of 2 levels
data$acuity <- as.factor(data$acuity)
set.seed(199)
trainIndex <- createDataPartition(data$acuity, p = .7, list=F)
train_data <- data[trainIndex,]
test_data <- data[-trainIndex,]
# checking for imbalances
d.table<- table(data$acuity)
prop.table(d.table)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=5, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(acuity ~ ., data=train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
varImp(d.log)
d.log
#calculate resampled accuracy/confusion matrix using extracted predictions from resampling
confusionMatrix(d.log$pred$pred, d.log$pred$obs) #take averages
##linear discriminant analysis
set.seed(199)
d.lda <-  train(acuity ~ ., data=train_data, method="lda", family="binomial", metric="ROC", trControl=ctrl)
d.lda
varImp(d.lda)
confusionMatrix(d.lda$pred$pred, d.lda$pred$obs) #take averages
#k nearest neighbors classification
set.seed(199)
d.knn <-  train(acuity ~ ., data=train_data, method="knn", metric="ROC", trControl=ctrl, tuneLength=3) #let caret decide 10 best parameters to search
?resamples
?confusionMatrix
?train
?family
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(acuity ~ ., data=train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
varImp(d.log)
d.log
#calculate resampled accuracy/confusion matrix using extracted predictions from resampling
confusionMatrix(d.log$pred$pred, d.log$pred$obs) #take averages
# relevel the yes category as the positive class
data$acuity <- relevel(data$acuity, ref="Yes")
set.seed(199)
trainIndex <- createDataPartition(data$acuity, p = .7, list=F)
train_data <- data[trainIndex,]
test_data <- data[-trainIndex,]
# checking for imbalances
d.table<- table(data$acuity)
prop.table(d.table)
#setup control function for resampling and binary classification performance
#using 10 fold cross validation
ctrl <- trainControl(method = "cv", number=10, summaryFunction=twoClassSummary,
classProbs=T, savePredictions=T) #saving predictions from each resample fold
##logistic regression
set.seed(199)#ALWAYS USE same SEED ACROSS trains to ensure identical cv folds
d.log <-  train(acuity ~ ., data=train_data, method="glm", family="binomial", metric="ROC", trControl=ctrl)
summary(d.log)
varImp(d.log)
d.log
#calculate resampled accuracy/confusion matrix using extracted predictions from resampling
confusionMatrix(d.log$pred$pred, d.log$pred$obs) #take averages
